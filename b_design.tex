\chapter{Design discussion}\label{app:design}
This appendix contains some ideas that are being designed for the future versions of the Dual programming language.

\section{Comments}\label{sec:comments}
If multiline comments were implemented as expressions on parser-level then, in
combination with \texttt{|} special character we could have one-word comments,
which could be useful for describing arguments to facilitate reading of
expressions. For example we could implement list comprehensions, where:
\begin{lstlisting}
$<-[^[x 2] x range[0 10]] $<-[$[x y] x $[1 2 3] y $[3 1 4] <>[x y]]
\end{lstlisting}
would be equivalent to
Python's\cite[Section~5.1.3]{python_tutorial}:
\begin{lstlisting}
[x**2 for x in range(10)] [(x, y) for x in [1,2,3] for y in [3,1,4] if x !=
y]
\end{lstlisting}
As we see this notation is acceptable (if not cleaner) for simple
comprehensions, but starts being less readable for complex ones. This could be
alleviated by introducing one-word comments:
\begin{lstlisting}
$<-[^[x 2] --|for x --|in range[0 10]]

$<-[$[x y] --|for x --|in $[1 2 3] --|for y --|in $[3 1 4] --|if <>[x y]]
\end{lstlisting}
which are easily inserted inline with code and have a benefit of clearly
separating individual parts of an expression, because of being easily
distinguished visually from the rest. This can simulate different syntactical
constructs from other programming languages, like:
\begin{lstlisting}
if [>[a b] --|then log['|greater] --|else log['|lesser-or-equal] ]
\end{lstlisting}
Except that it is not validated by the parser. But we could imagine a separate
or extend the existing syntax analyzer, so it could validate such ``keyword''
comments or even use them in some way. For example, we could add a static type
checker to the language -- in a similar manner that TypeScript or
Flow\cite{flow_site} extends JavaScript. This would be
completely transparent to the rest of the language, so any program that uses
this feature would be valid without it and it could be turned on and off as
needed.

To reduce the number of characters that have to be typed, we could decide to use
a different comment ``operator'', such as \texttt{\%}:
\begin{lstlisting}
$<-[^[x 2] %|for x %|in range[0 10]]

$<-[$[x y] %|for x %|in $[1 2 3] %|for y %|in $[3 1 4] %|if <>[x y]]

if [>[a b] %|then log['|greater] %|else log['|lesser-or-equal] ]
\end{lstlisting}

Or even, at the cost of complicating the parser, introduce a separate syntax for
one-word comments:
\begin{lstlisting}
-- `%:type` could be a type annotation
bind [a 3 %:integer]
bind [b 5 %:integer]

-- will print "lesser-or-equal"
if [>[a b] %then
log['|greater]
%else
log['|lesser-or-equal]
]
\end{lstlisting}

In future versions of the language, comments will be stored separately from
whitespace in the EST. This enables easy smart indentation -- only a prefix of
the relevant expression has to be looked at, no need to filter out comments. It
also enables using comments structurally, as a metalanguage for annotations,
documentation, etc.

\section{C-like syntax}
Throughout this thesis I introduced multiple ways in which the basic, Lisp-like
syntax of Dual can be easily extended with simple enhancements, such as adding
more general-purpose special characters, macros, single-word comments (as
described in Section \ref{sec:comments}), etc.

Going further along this path, keeping in mind that a real-world language should
appeal to its users we find ourselves introducing more and more elements of
C-like syntax. This section describes more possible ways in which the simple
syntax could be morphed to resemble the most popular languages.  Ultimately all
this could be implemented with a conventional complex parser for a C-like
language that translates to bare Dual syntax.

Below I present a snapshot from one of designs I have been working on in order
to achieve some goals described in this section:
\begin{lstlisting}
fit map" {f; lst} { let {i; ret} [0, []];

while ((i < lst.length)) { ret.push f(lst i); set i" ((i + 1)) }; ret };
\end{lstlisting}

This would be equivalent to:
\begin{lstlisting}
bind ['|map of ['|f '|lst do [ bind ['[i ret] $[0 $[]]]

while [<[i lst|length] do [ ret[push][f[lst|@[i]]] mutate* ['|i +[i 1]]
]] ret ]]]
\end{lstlisting}

Using the notation presented in Chapter \ref{chap:lang}.

One may observe that:
\begin{itemize}
    \item The syntax is much richer, somewhat C-like, but with critical
    differences, reflecting significantly different nature of the language. At
    a first glance, it has a familiar look defined by blocks of code delimited
    by curly-braces, inside which statements (actually expressions) are
    separated by semicolons; there are different kinds of bracketing
    characters (\texttt{\{\}()[]}) with different meanings (described below)
    \item Names of the primitives are \textit{full} English words, although as
    short as possible. \texttt{let} introduces a variable definition --
    similarly to \texttt{bind}. \texttt{fit <name> <args> <body>} is a
    shorthand for \texttt{let <name> (of <args> <body>)}, where \texttt{of}
    produces a function value. This translates to \texttt{bind [<name> of
        [<args> <body>]]}.
    \item \texttt{\{\}} delimit a string; inside a string words are separated by
    \texttt{;}. Strings are stored in raw as well as structural (syntax tree)
    form. They are a way of quoting code. This provides an explicit laziness
    mechanism. One-word strings are denoted with \texttt{"} at the end of the
    word, which resembles the mathematical double prime notation.
    \item \texttt{[]} delimit list literals; inside list literals, elements are
    separated by \texttt{,}. Lists are a basic data structure. They are
    actually objects, somewhat like in JavaScript. If a list contains at least
    one \texttt{:} character (not shown in the example), it will be validated
    as key-value container; if it doesn't, it will be treated as array with
    integer-based indices
    \item \texttt{()} are used in function invocations; \texttt{f(a, b, c)}
    translates to \texttt{f[a b c]}; \texttt{,} separates function arguments;
    \texttt{f x} is a shorthand notation for \texttt{f(x)}. This, in
    combination with currying primitives into appropriate macros allows for
    elimination of excessive brackets and separators. Invocations of
    primitives resemble use of keywords from other lanugages.
    \item But at the same time primitives are defined as regular functions --
    they are no longer treated exceptionally by the interpreter. When they are
    invoked, all of their arguments are first evaluated. This works, because
    now it is required that the programmer quote any words that shouldn't be
    evaluated, such as identifier names when using \texttt{let}. So primitives
    are just regular functions operating on code, thanks to the explicit
    laziness provided by strings.
    \item \texttt{(())} introduce an infix expression, which respects basic
    operator precedence: (\texttt{((a + b * 2))} would translate to
    \texttt{+[a *[b 2]]}. This could be implemented with a separate parser
    based on the
    shunting-yard\cite{shunting_yard} or
    similar algorithm that is triggered by the \texttt{((} sequence. It would
    translate these infix expressions to prefix form and return them back to
    the original parser.
\end{itemize}